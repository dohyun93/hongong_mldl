{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfioF2NKkCY76o2DPJ8bAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohyun93/hongong_mldl/blob/main/7_2_%EC%8B%AC%EC%B8%B5_%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "55a46Mp6Fevr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb689b40-2a85-4db6-d067-d9c5b6390b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 7-1 장에서 살펴본 인공신경망은 히든레이어가 1개인 신경망이었다.\n",
        "# 입력층과 출력층 사이에 층을 더 추가해서 심층 신경망을 만들어보자.\n",
        "\n",
        "from tensorflow import keras\n",
        "(train_input, train_target), (test_input, test_target) =\\\n",
        "keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_scaled = train_input / 255.0 # 1. 표준화\n",
        "train_scaled = train_scaled.reshape(-1, 28*28) # 2. 각 그림을 1차원 벡터로 만듬.\n",
        "\n",
        "# 3. 훈련 세트에서 다시 훈련/검증 세트로 나눔.\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "jC3oZsl9GfET"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 인공신경망 모델에 층을 추가해보자.\n",
        "\n",
        "# 입력층: (n_samples, 28*28)\n",
        "# h1: (28*28, 100) -> 뉴런 수가 100개인 히든레이어 h1. 활성화함수는 시그모이드/렐루 같은걸 사용할 수 있음.\n",
        "# 출력층: (100, 10) -> 다중 분류이므로 소프트맥스 함수를 활성화 함수로 적용. 이진분류는 시그모이드함수.\n",
        "\n",
        "# 회귀의 경우 출력층에 활성화함수를 적용하지 않는다. 그 이유는 분류는 활성화함수로 확률로의 변환이 필요하지만, 회귀는 \n",
        "# 임의의 숫자를 추론하는 모델이므로 적용하지 않는다.\n",
        "\n",
        "# 여기서 은닉층에 활성화함수를 사용하는 이유를 살펴보면 아래 예시를 확인해보자.\n",
        "# a * 3 - 1 = b\n",
        "# 2 * b + 3 = c 라고 했을 때, 활성화 함수를 적용하지 않는 경우라고 볼 수 있다.\n",
        "# 결국 c = 6 * a + 1 이되며 b가 하는 일이 없는거나 마찬가지다.\n",
        "# 이러한 선형 방정식을 적절히 비선형으로 뒤틀어주어야 한다. 그래야 이전 히든레이어가 존재하는 의미가 생기기 때문이다.\n",
        "\n",
        "# a * 3 - 1 = b\n",
        "# log(b) = k\n",
        "# 2 * k + 3 = c\n",
        "# 처럼.\n",
        "\n",
        "# 자 그럼 이제 은닉층의 활성화함수로 시그모이드 함수를, 출력층의 활성화 함수로 소프트맥스 함수를 사용한 인공신경망을 만들어보자.\n",
        "# 히든레이어부터 출력층까지 만든다.\n",
        "dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(28*28,))\n",
        "dense2 = keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "# dense1, dense2 객체를 Sequential 클래스에 추가하여 심층 신경망을 만들어보자.\n",
        "model = keras.Sequential([dense1, dense2])\n",
        "# keras는 모델의 summary() 메서드로 층에 대한 유용한 정보를 보여준다.\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM_3xEQ3HOhM",
        "outputId": "b7b2fa74-d425-4c0c-ddb8-b0cdbe6867c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense1, dense2 객체를 만들어서 keras.Sequential 메서드에 리스트로 전달하여 신경망을 만들었지만,\n",
        "# 일반적으로 아래 방법으로 케라스로 심층신경망을 만든다.\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden1'),\n",
        "    keras.layers.Dense(10, activation='softmax', name='output')\n",
        "], name='패션 Mnist 모델')\n",
        "\n",
        "model.summary()\n",
        "# dense1, dense2 같은 객체를 따로 저장해서 쓸 일이 없기 때문에 위 처럼 하는게 일반적이다.\n",
        "# 참고로 모델 파라미터 개수가 78500 인 이유는 784 입력층 X 히든레이어 뉴런 100개 곱인 78400개에\n",
        "# 히든레이어 절편 100개 를 더했기 때문이다."
      ],
      "metadata": {
        "id": "V9Ovu-XmL-te",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbb3deb-e895-4f94-c486-b904fe9d8d77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"패션 Mnist 모델\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hidden1 (Dense)             (None, 100)               78500     \n",
            "                                                                 \n",
            " output (Dense)              (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하지만, 이런 히든레이어가 하나면 상관없지만, 만약 10개가 있다고 할 경우, 모델 선언이 너무 길어지고 복잡해진다.\n",
        "# 아래 방법이 가장 널리 사용된다.\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(100, activation='softmax', input_shape=(784,)))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkqUyJYUE1J1",
        "outputId": "75aad5e4-8c68-454a-fde2-7765713a888f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제, 만든 케라스 DNN 모델을 갖고 훈련해보자.\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1DKgZn6Fr2-",
        "outputId": "40efcc3e-6a25-4025-9e95-7aeba5be8e13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6081 - accuracy: 0.7623\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5910 - accuracy: 0.7739\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5760 - accuracy: 0.7839\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5638 - accuracy: 0.7887\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5531 - accuracy: 0.7961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a1f67cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid 말고 이미지 분류에서 뛰어난 성능을 낼 수 있는 활성화 함수인 'ReLU' 에 대해 알아보자.\n",
        "\n",
        "# 1. sigmoid 활성화 함수의 한계\n",
        "# 시그모이드 함수는 절편 0.5를 기준으로 오른쪽으로 갈 수록, 왼 쪽으로 갈수록 활성화 함수의 값이 각각 1, 0에 가깝다.\n",
        "# 그래서 심층 신경망일 수록 은닉층을 거칠 때 마다 출력되는 값의 변화가 거의 없어서 모델파라미터 업데이트가 느린 문제가 있다.\n",
        "\n",
        "# 이를 보완하기 위해 렐루(ReLU) 라는 활성 함수가 제안되었다.\n",
        "# ReLU(z) = max(0, z)\n",
        "# 입력이 0이하면 0, 그 이상이면 그대로 출력하는 활성화 함수다.\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28))) # 1. 입력층 바로 뒤 flatten 층을 둔다. 은닉층에 들어가기 전 Reshape이 아닌 flatten 층을 넣어준다.\n",
        "model.add(keras.layers.Dense(100, activation='relu')) # 2. 은닉층\n",
        "model.add(keras.layers.Dense(10, activation='softmax')) # 3. 출력층\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCTyEBOOGG_6",
        "outputId": "67c50b0f-c5b5-44a6-bed5-6d0941c39c92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 Summary() 해석\n",
        "# flatten 층은 모델 파라미터가 없다. 단순히 입력되는 데이터를 벡터로 shape 변환이 임무.\n",
        "# flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다는 장점이 있다.\n",
        "# 입력 데이터에 대한 전처리 과정을 가능한 한 모델에 포함시키는 것이 케라스 API 의 철학 중 하나다.\n",
        "\n",
        "(train_input, train_target), (test_input, test_targer) =\\\n",
        "keras.datasets.fashion_mnist.load_data()\n",
        "train_scaled = train_input / 255.0\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "2gCc_n2nJG98"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jhGLX0RJ0Xw",
        "outputId": "b44b7c84-ceb6-4657-ebd1-ce6f6b4baa53"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5285 - accuracy: 0.8127\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3900 - accuracy: 0.8601\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3533 - accuracy: 0.8740\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3343 - accuracy: 0.8802\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3161 - accuracy: 0.8876\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a1d60ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid 활성함수를 신경망의 활성화 함수로 적용한 성능 대비, 렐루 함수를 적용했을 때가 성능이 더 높은 걸 알 수 있다.\n",
        "# 검증 세트에서도 확인해보자.\n",
        "\n",
        "model.evaluate(val_scaled, val_target)\n",
        "\n",
        "# 검증 세트에서도 훈련세트 대비 비슷한 성능을 보임을 알 수 있다.\n",
        "# epochs=5 로 5번의 에포크 동안 훈련했는데, 더 훈련하지 않을 이유가 없다.\n",
        "# 그 전에 인공신경망의 하이퍼파라미터에 대해 좀 더 알아보자."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9uml-2rKBqV",
        "outputId": "fc6b9894-54b4-4fbd-ec8c-1b9871d23eb0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 0s 964us/step - loss: 0.3590 - accuracy: 0.8752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3590386211872101, 0.875249981880188]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 옵티마이저\n",
        "\n",
        "> 모델 파라미터는 인공신경망이 학습하는 인공신경망의 가중치로, 이는 사람이 관여할 사항이 아니다.\n",
        "\n",
        "> 하이퍼 파라미터는 인공신경망이 학습하는 대상이 아닌, 개발자가 지정하는 설정값이다. \n",
        "\n",
        "> 하이퍼 파라미터 예시: ```epoch수```, ```은닉층 뉴런수```, ```층의종류(DNN, CNN, ...)```, ```은닉층 개수```, ```미니배치사이즈```, ```경사하강알고리즘(=옵티마이저) 종류(RMSprop, SGD, ...)```, ```학습률(learning rate)```"
      ],
      "metadata": {
        "id": "4b55lNxiKP1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 옵티마이저를 활용해보자.\n",
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "\n",
        "# 'sgd' 로 optimizer 매개변수를 전달해도 되지만, 아래처럼 명시적으로 SGD 객체를 전달해줘도 된다.\n",
        "sgd = keras.optimizers.SGD()\n",
        "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "\n",
        "# SGD 클래스의 디폴트 학습률 (learning_rate) 가 0.01 이라고 할 때, 이를 바꾸고 싶다면 learning_rate 매개변수에 저장한다.\n",
        "# sgd = keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "Vi9Je4GHM1vH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저의 종류 2가지\n",
        "# 1. 기본 경사하강 옵티마이저 (lr 고정)\n",
        "# 1-1) SGD (momentum 디폴트값은 0)\n",
        "# 1-2) momentum (SGD 클래스에서 momentum > 0 일 때 이전의 그래디언트를 가속도로 활용)\n",
        "# 1-3) nesterov momentum (SGD 클래스에서 nesterov=True 로 바꾸면 네스테로프모멘텀 최적화를 수행. 모멘텀최적화를 2번 반복하는 방식)\n",
        "sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
        "# 일반적으로 nesterov 모멘텀 최적화가 기본 확률적 경사하강법보다 더 나은 성능을 제공..\n",
        "\n",
        "# 2. 적응적 학습률 옵티마이저 (lr 변동)\n",
        "# 모델이 최적점에 가까이 갈 수록, 학습률을 낮추어 안정적으로 성능 수렴할 수 있는 옵티마이저.\n",
        "# 아래 세 알고리즘 모두 디폴트 학습률은 0.001이다. (1e-3)\n",
        "# 2-1) RMSprop \n",
        "rmsprop = keras.optimizers.RMSprop()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=rmsprop, metrics='accuracy')\n",
        "# 2-2) Adagrad\n",
        "adagrad = keras.optimizers.Adagrad()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=adagrad, metrics='accuracy')\n",
        "\n",
        "# 2-3) Adam : RMSprop + momentum\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(keras.layers.Dense(100, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sCV6-SBOiqT",
        "outputId": "c59627e0-96b9-49b8-85f3-067fd28ded3c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5228 - accuracy: 0.8160\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3930 - accuracy: 0.8591\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3516 - accuracy: 0.8729\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3237 - accuracy: 0.8821\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3066 - accuracy: 0.8870\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a1a4ce50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop 을 옵티마이저로 지정했을 때나 Adam 이나 훈련세트의 점수는 거의 비슷하다.\n",
        "# 검증 세트에 대해서도 확인해보자.\n",
        "model.evaluate(val_scaled, val_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5S8vt8xRdEt",
        "outputId": "5492b590-bffa-4bd5-b15a-de32ac4c8dfc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 0s 889us/step - loss: 0.3341 - accuracy: 0.8799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33405858278274536, 0.8799166679382324]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 마찬가지로 비슷하다."
      ],
      "metadata": {
        "id": "DQ7MgLN2RngG"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}